---
---

@article{2210.06516v2,
  author        = {Yi Zeng and Minzhou Pan and Himanshu Jahagirdar and Ming Jin and Lingjuan Lyu and Ruoxi Jia},
  title         = {How to Sift Out a Clean Data Subset in the Presence of Data Poisoning?},
  eprint        = {2210.06516v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {External data sources are increasingly being used to train
                  machine learning (ML) models as the data demand increases.
                  However, the integration of external data into training poses
                  data poisoning risks, where malicious providers manipulate
                  their data to compromise the utility or integrity of the model.
                  Most data poisoning defenses assume access to a set of clean
                  data (referred to as the base set), which could be obtained
                  through trusted sources. But it also becomes common that
                  entire data sources for an ML task are untrusted (e.g., Internet
                  data). In this case, one needs to identify a subset within a
                  contaminated dataset as the base set to support these defenses.
                  This paper starts by examining the performance of defenses
                  when poisoned samples are mistakenly mixed into the base
                  set. We analyze five representative defenses that use base sets
                  and find that their performance deteriorates dramatically with
                  less than 1% poisoned points in the base set. These findings
                  suggest that sifting out a base set with high precision is key to
                  these defenses' performance. Motivated by these observations,
                  we study how precise existing automated tools and human
                  inspection are at identifying clean data in the presence of data
                  poisoning. Unfortunately, neither effort achieves the precision
                  needed that enables effective defenses. Worse yet, many of the
                  outcomes of these methods are worse than random selection.
                  In addition to uncovering the challenge, we take a step further and propose a practical countermeasure, META-SIFT .
                  Our method is based on the insight that existing poisoning attacks shift data distributions, resulting in high prediction loss
                  when training on the clean portion of a poisoned dataset and
                  testing on the corrupted portion. Leveraging the insight, we
                  formulate a bilevel optimization to identify clean data and further introduce a suite of techniques to improve the efficiency
                  and precision of the identification. Our evaluation shows that
                  META-SIFT can sift a clean base set with 100% precision
                  under a wide range of poisoning threats. The selected base
                  set is large enough to give rise to successful defense when
                  plugged into the existing defense techniques.},
  year          = {2023},
  month         = {May},
  url           = {https://arxiv.org/abs/2210.06516v2},
  file          = {2210.06516v2.pdf},
  eprintnover   = {2210.06516}
}

@article{1602.05629,
  author        = {H. Brendan McMahan and Eider Moore and Daniel Ramage and Seth Hampson and Blaise Agüera y Arcas},
  title         = {Communication-Efficient Learning of Deep Networks from Decentralized Data},
  eprint        = {1602.05629},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {Modern mobile devices have access to a wealth
                  of data suitable for learning models, which in turn
                  can greatly improve the user experience on the
                  device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos.
                  However, this rich data is often privacy sensitive,
                  large in quantity, or both, which may preclude
                  logging to the data center and training there using
                  conventional approaches. We advocate an alternative that leaves the training data distributed on
                  the mobile devices, and learns a shared model by
                  aggregating locally-computed updates. We term
                  this decentralized approach Federated Learning.
                  We present a practical method for the federated
                  learning of deep networks based on iterative
                  model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments
                  demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a
                  defining characteristic of this setting. Communication costs are the principal constraint, and
                  we show a reduction in required communication
                  rounds by 10-100x as compared to synchronized
                  stochastic gradient descent.},
  year          = {2016},
  month         = {Feb},
  url           = {https://arxiv.org/abs/1602.05629},
  file          = {1602.05629.pdf},
  eprintnover   = {1602.05629}
}

@article{1708.06733,
  author        = {Tianyu Gu and Brendan Dolan-Gavitt and Siddharth Garg},
  title         = {BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain},
  eprint        = {1708.06733},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {Deep learning-based techniques have achieved stateof-the-art performance on a wide variety of recognition and
                  classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation
                  on many GPUs; as a result, many users outsource the training
                  procedure to the cloud or rely on pre-trained models that
                  are then fine-tuned for a specific task. In this paper we
                  show that outsourced training introduces new security risks:
                  an adversary can create a maliciously trained network (a
                  backdoored neural network, or a BadNet) that has state-of-theart performance on the user's training and validation samples,
                  but behaves badly on specific attacker-chosen inputs. We first
                  explore the properties of BadNets in a toy example, by creating
                  a backdoored handwritten digit classifier. Next, we demonstrate
                  backdoors in a more realistic scenario by creating a U.S. street
                  sign classifier that identifies stop signs as speed limits when
                  a special sticker is added to the stop sign; we then show in
                  addition that the backdoor in our US street sign detector can
                  persist even if the network is later retrained for another task
                  and cause a drop in accuracy of 25% on average when the
                  backdoor trigger is present. These results demonstrate that
                  backdoors in neural networks are both powerful and—because
                  the behavior of neural networks is difficult to explicate—
                  stealthy. This work provides motivation for further research
                  into techniques for verifying and inspecting neural networks,
                  just as we have developed tools for verifying and debugging
                  software.},
  year          = {2017},
  month         = {Aug},
  url           = {https://arxiv.org/abs/1708.06733},
  file          = {1708.06733.pdf},
  eprintnover   = {1708.06733}
}

@article{1512.03385,
  author        = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title         = {Deep Residual Learning for Image Recognition},
  eprint        = {1512.03385},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {Deeper neural networks are more difficult to train. We
                  present a residual learning framework to ease the training
                  of networks that are substantially deeper than those used
                  previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual
                  networks are easier to optimize, and can gain accuracy from
                  considerably increased depth. On the ImageNet dataset we
                  evaluate residual nets with a depth of up to 152 layers—8×
                  deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error
                  on the ImageNet test set. This result won the 1st place on the
                  ILSVRC 2015 classification task. We also present analysis
                  on CIFAR-10 with 100 and 1000 layers.
                  The depth of representations is of central importance
                  for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep
                  residual nets are foundations of our submissions to ILSVRC
                  & COCO 2015 competitions1
                  , where we also won the 1st
                  places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  year          = {2015},
  month         = {Dec},
  url           = {https://arxiv.org/abs/1512.03385},
  file          = {1512.03385.pdf},
  eprintnover   = {1512.03385}
}