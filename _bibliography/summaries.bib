---
---

@article{2210.06516v2,
  author        = {Yi Zeng and Minzhou Pan and Himanshu Jahagirdar and Ming Jin and Lingjuan Lyu and Ruoxi Jia},
  title         = {How to Sift Out a Clean Data Subset in the Presence of Data Poisoning?},
  eprint        = {2210.06516v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {External data sources are increasingly being used to train
                  machine learning (ML) models as the data demand increases.
                  However, the integration of external data into training poses
                  data poisoning risks, where malicious providers manipulate
                  their data to compromise the utility or integrity of the model.
                  Most data poisoning defenses assume access to a set of clean
                  data (referred to as the base set), which could be obtained
                  through trusted sources. But it also becomes common that
                  entire data sources for an ML task are untrusted (e.g., Internet
                  data). In this case, one needs to identify a subset within a
                  contaminated dataset as the base set to support these defenses.
                  This paper starts by examining the performance of defenses
                  when poisoned samples are mistakenly mixed into the base
                  set. We analyze five representative defenses that use base sets
                  and find that their performance deteriorates dramatically with
                  less than 1% poisoned points in the base set. These findings
                  suggest that sifting out a base set with high precision is key to
                  these defenses' performance. Motivated by these observations,
                  we study how precise existing automated tools and human
                  inspection are at identifying clean data in the presence of data
                  poisoning. Unfortunately, neither effort achieves the precision
                  needed that enables effective defenses. Worse yet, many of the
                  outcomes of these methods are worse than random selection.
                  In addition to uncovering the challenge, we take a step further and propose a practical countermeasure, META-SIFT .
                  Our method is based on the insight that existing poisoning attacks shift data distributions, resulting in high prediction loss
                  when training on the clean portion of a poisoned dataset and
                  testing on the corrupted portion. Leveraging the insight, we
                  formulate a bilevel optimization to identify clean data and further introduce a suite of techniques to improve the efficiency
                  and precision of the identification. Our evaluation shows that
                  META-SIFT can sift a clean base set with 100% precision
                  under a wide range of poisoning threats. The selected base
                  set is large enough to give rise to successful defense when
                  plugged into the existing defense techniques.},
  year          = {2023},
  month         = {May},
  url           = {https://arxiv.org/abs/2210.06516v2},
  file          = {2210.06516v2.pdf},
  eprintnover   = {2210.06516}
}

@article{2210.12873v2,
  author        = {Kaiyuan Zhang and Guanhong Tao and Qiuling Xu and Siyuan Cheng and Shengwei An and Yingqi Liu and Shiwei Feng and Guangyu Shen and Pin-Yu Chen and Shiqing Ma and Xiangyu Zhang},
  title         = {FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated Learning},
  eprint        = {2210.12873v2},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {Federated Learning (FL) is a distributed learning paradigm that enables different
                  parties to train a model together for high quality and strong privacy protection. In
                  this scenario, individual participants may get compromised and perform backdoor
                  attacks by poisoning the data (or gradients). Existing work on robust aggregation
                  and certified FL robustness does not study how hardening benign clients can affect
                  the global model (and the malicious clients). In this work, we theoretically analyze
                  the connection among cross-entropy loss, attack success rate, and clean accuracy
                  in this setting. Moreover, we propose a trigger reverse engineering based defense
                  and show that our method can achieve robustness improvement with guarantee
                  (i.e., reducing the attack success rate) without affecting benign accuracy. We conduct comprehensive experiments across different datasets and attack settings. Our
                  results on nine competing SOTA defense methods show the empirical superiority
                  of our method on both single-shot and continuous FL backdoor attacks. Code is
                  available at https://github.com/KaiyuanZh/FLIP.},
  year          = {2023},
  month         = {Feb},
  url           = {https://arxiv.org/abs/2210.12873v2},
  file          = {2210.12873v2.pdf},
  eprintnover   = {2210.12873}
}

@article{2304.02786,
  author        = {Zhenting Wang and Kai Mei and Juan Zhai and Shiqing Ma},
  title         = {UNICORN: A Unified Backdoor Trigger Inversion Framework},
  eprint        = {2304.02786},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {The backdoor attack, where the adversary uses inputs stamped with triggers (e.g.,
                  a patch) to activate pre-planted malicious behaviors, is a severe threat to Deep
                  Neural Network (DNN) models. Trigger inversion is an effective way of identifying backdoor models and understanding embedded adversarial behaviors. A
                  challenge of trigger inversion is that there are many ways of constructing the
                  trigger. Existing methods cannot generalize to various types of triggers by making certain assumptions or attack-specific constraints. The fundamental reason
                  is that existing work does not consider the trigger's design space in their formulation of the inversion problem. This work formally defines and analyzes the
                  triggers injected in different spaces and the inversion problem. Then, it proposes a unified framework to invert backdoor triggers based on the formalization of triggers and the identified inner behaviors of backdoor models from our
                  analysis. Our prototype UNICORN is general and effective in inverting backdoor triggers in DNNs. The code can be found at https://github.com/
                  RU-System-Software-and-Security/UNICORN.},
  year          = {2023},
  month         = {Apr},
  url           = {https://arxiv.org/abs/2304.02786},
  file          = {2304.02786.pdf},
  eprintnover   = {2304.02786}
}

@article{1602.05629,
  author        = {H. Brendan McMahan and Eider Moore and Daniel Ramage and Seth Hampson and Blaise Agüera y Arcas},
  title         = {Communication-Efficient Learning of Deep Networks from Decentralized Data},
  eprint        = {1602.05629},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {Modern mobile devices have access to a wealth
                  of data suitable for learning models, which in turn
                  can greatly improve the user experience on the
                  device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos.
                  However, this rich data is often privacy sensitive,
                  large in quantity, or both, which may preclude
                  logging to the data center and training there using
                  conventional approaches. We advocate an alternative that leaves the training data distributed on
                  the mobile devices, and learns a shared model by
                  aggregating locally-computed updates. We term
                  this decentralized approach Federated Learning.
                  We present a practical method for the federated
                  learning of deep networks based on iterative
                  model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments
                  demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a
                  defining characteristic of this setting. Communication costs are the principal constraint, and
                  we show a reduction in required communication
                  rounds by 10-100x as compared to synchronized
                  stochastic gradient descent.},
  year          = {2016},
  month         = {Feb},
  url           = {https://arxiv.org/abs/1602.05629},
  file          = {1602.05629.pdf},
  eprintnover   = {1602.05629}
}

@article{1708.06733,
  author        = {Tianyu Gu and Brendan Dolan-Gavitt and Siddharth Garg},
  title         = {BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain},
  eprint        = {1708.06733},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  abstract      = {Deep learning-based techniques have achieved stateof-the-art performance on a wide variety of recognition and
                  classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation
                  on many GPUs; as a result, many users outsource the training
                  procedure to the cloud or rely on pre-trained models that
                  are then fine-tuned for a specific task. In this paper we
                  show that outsourced training introduces new security risks:
                  an adversary can create a maliciously trained network (a
                  backdoored neural network, or a BadNet) that has state-of-theart performance on the user's training and validation samples,
                  but behaves badly on specific attacker-chosen inputs. We first
                  explore the properties of BadNets in a toy example, by creating
                  a backdoored handwritten digit classifier. Next, we demonstrate
                  backdoors in a more realistic scenario by creating a U.S. street
                  sign classifier that identifies stop signs as speed limits when
                  a special sticker is added to the stop sign; we then show in
                  addition that the backdoor in our US street sign detector can
                  persist even if the network is later retrained for another task
                  and cause a drop in accuracy of 25% on average when the
                  backdoor trigger is present. These results demonstrate that
                  backdoors in neural networks are both powerful and—because
                  the behavior of neural networks is difficult to explicate—
                  stealthy. This work provides motivation for further research
                  into techniques for verifying and inspecting neural networks,
                  just as we have developed tools for verifying and debugging
                  software.},
  year          = {2017},
  month         = {Aug},
  url           = {https://arxiv.org/abs/1708.06733},
  file          = {1708.06733.pdf},
  eprintnover   = {1708.06733}
}